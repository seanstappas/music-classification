\documentclass[a4paper,titlepage]{article}
\usepackage[utf8]{inputenc}
\usepackage{fullpage}
\usepackage{indentfirst}
\usepackage[per-mode=symbol]{siunitx}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{color}
\usepackage{amsmath}
\usepackage{array}
\usepackage[hidelinks]{hyperref}
\usepackage[format=plain,font=it]{caption}
\usepackage{subcaption}
\usepackage{standalone}
\usepackage[nottoc]{tocbibind}
\usepackage{cleveref}
\usepackage{titlesec}
\usepackage{booktabs}
\usepackage{csvsimple}
\usepackage[super]{nth}

% Custom commands
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
\newcommand{\code}[1]{\texttt{#1}}
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}

\titleformat*{\section}{\normalsize\bfseries}

%opening
\title{
	\textbf{ECSE 526 \\ Assignment 2}
	\\ \large Music Genre Classification
}
\author{Sean Stappas \\ 260639512}
\date{October \nth{19}, 2017}

\begin{document}
	\sloppy
	\maketitle
	\twocolumn
	
	\section*{Introduction}
	
	Basic Gaussian and simple k-nearest neighbor (kNN) classifiers were created to classify songs into ten distinct genres. To improve performance for the online Kaggle competition, multiple elements were used from the scikit-learn (sklearn) Python library. These include using their implementation of a k-d tree for kNN, as well as using many of their other classifiers, like the SVM, Naive Bayes, Multi-layer Perceptron, ADA, QDA and Gaussian Process classifiers. The focus of this report will be to compare the simple Gaussian and kNN classifiers.
	
	\section{What assumptions about the data do we make when we model the data using a Gaussian distribution?}
	% Several key assumptions are stated.	
	% TODO: See 14.3, 20.2.3
	% Good reference: http://scikit-learn.org/stable/modules/neighbors.html
	
	Fundamentally, when we model the data using a Gaussian distribution, we assume that it has a Gaussian shape. This mainly implies the following:
	
	\begin{enumerate}
		\item The data has a mean.
		\item The data has a standard deviation.
		\item The data is symmetric around its mean.
	\end{enumerate}
	
	Indeed, a Gaussian distribution is uniquely defined by its standard deviation and its mean. Although the value of the distribution is technically non-zero everywhere, it will be practically zero when the distance of a value from the mean is more than a couple standard deviations.
	
	\section{When do you expect that a Gaussian will work well and when do you think it will not work well?}
	% Several situations are given, with elaborate discussion of the factors that make one classifier better than the other.
	
	A Gaussian classifier will work well when the data can be modeled with a Gaussian distribution, with the features listed in the previous question.
	
	A Gaussian classifier will not work in many situations. First, if the data itself cannot be modeled as Gaussian (using the assumptions in Question 1), then a Gaussian classifier will clearly not predict the correct values. For example, if the distribution is skewed in one direction or not symmetric, a Gaussian model will not work well.
	
	Another example where a Gaussian model would not be appropriate is if there are multiple clusters of values. Here, multiple Gaussian distributions may be appropriate, but certainly not one.
	
	Also, if the feature vector distributions of two categories are very similar, then their corresponding Gaussian distributions will be very close, perhaps intersecting. In this situation, a Gaussian classifier can have a hard time differentiating between these two categories.
	
	Also, a Gaussian classifier will not work well for values that stray too far from the mean or with many outliers. In situations like this, kNN has the clear advantage.
	
	One advantage that the Gaussian classifier does have is it should be much faster at predicting categories for new observations, since, unlike kNN, it does not have to search through the entire space of previously trained data.
	
	\section{What values of $k$ work best for the kNN classifier?}
	% A value of k is stated, with a graph showing the results of different experiments.

	After testing on various values of $k$, the best one was found to be 
	
	% TODO: Do k-fold cross-validation
	% TODO: Show plot + table showing values of k vs accuracy, choose best one
	
	\section{Based on your results from this assignment, which classifier (Gaussian or kNN) works best for the task of Music Genre Classification? Why?}
	% A classifier is stated as best, with elaboration on why, and with supporting evidence.
	
	With local testing on the provided test song data, an accuracy of $x\%$ was achieved for the Gaussian classifier and $y\%$ for the kNN classifier ($k = 1$). Similarly, on the Kaggle competition data set, an accuracy of $29.098\%$ was achieved for the Gaussian classifier and $57.786\%$ for the kNN classifier ($k = 1$). This implies that the kNN classifier is better for music classification. The reasons why will now be explored.
	
	To have some more insight into where both classifiers are succeeding and failing, a confusion matrix was created for both the Gaussian and kNN classifiers. These can be seen in \Cref{table:confusion_gaussian,table:confusion_knn_1}, where each row in the table represents how many different genres were predicted (columns) for a given actual genre (row). We can see here that the kNN has relatively high numbers along the main diagonal of the matrix, which is a good sign. On the other hand, the Gaussian classifier mis-predicts many genres. It seems that almost all genres are predicted as ``rnb'', except for ``classical'' and ``jazz'', which are more accurately predicted.
	
	\begin{table*}[!htb]
		\centering
		\caption{Confusion matrix for the Gaussian classifier. Each row represents the actual genre, and each column the predicted genre. The ordering of the column and row genres are the same.}
		\csvautobooktabular[respect all]{csv/confusion_gaussian.csv}
		\label{table:confusion_gaussian}
	\end{table*}

	\begin{table*}[!htb]
		\centering
		\caption{Confusion matrix for the kNN classifier, with $k = 1$. Each row represents the actual genre, and each column the predicted genre. The ordering of the column and row genres are the same.}
		\csvautobooktabular[respect all]{csv/confusion_knn_1.csv}
		\label{table:confusion_knn_1}
	\end{table*}
	
	Most of the failures of the Gaussian classifier can be attributed to the simple fact that it retains much less information than the kNN for each genre. While the Gaussian classifier only stores a 12x12 covariance matrix and 12x1 mean vector for each genre, the kNN classifier stores every training example and uses these examples to predict every genre. This additional information can allow the kNN classifier to classify new songs with much greater accuracy.
		
	Also, kNN can take into account very complex distributions of feature vectors, whereas the Gaussian classifier will attempt to fit a Gaussian distribution to the data set, which may not be appropriate. This can be attributed to all the reasons discussed in Question 2.
	
	For example, non-symmetric or skewed data is hard to model with a Gaussian distribution. An example of this can be seen in \autoref{fig:metal_non_symmetric}, where two features were plotted against each other for ten songs from the ``metal'' genre.
	
	% Non-symmetric examples: metal4, metal5
	
	\begin{figure}[!htb]
		\centering
		\includegraphics[width=\columnwidth]{plots/metal_non_symmetric.pdf}
		\caption
		{Projection of feature 0 vs. feature 8 for 10 songs from the ``metal'' genre. The distribution is not symmetric.}
		\label{fig:metal_non_symmetric}
	\end{figure}

	Also, there are examples of genres where there are multiple clusters of data, which are hard to model with a single Gaussian. This can be seen in \autoref{fig:kids_cluster}, where two features were plotted against each other for ten songs from the ``kids'' genre.
	
	% Clustering examples: jazz19, jazz34, kids1, kids10, kids11, kids12, kids13, kids14, kids15, kids16, kids17
	
	\begin{figure}[!htb]
		\centering
		\includegraphics[width=\columnwidth]{plots/kids_cluster.pdf}
		\caption
		{Projection of feature 1 vs. feature 2 for 10 songs from the ``kids'' genre. Two separate clusters of values can be seen, on the left and the right.}
		\label{fig:kids_cluster}
	\end{figure}

	There are also genres with very similar distributions which cannot be easily differentiated with a Gaussian classifier. This can be seen in \autoref{fig:latin_rnb_similarity}, where the ``latin'' and ``rnb'' genres have very similar distributions. The similarity between the distributions for ``rnb'' and other genres is most probably why so many genres were predicted to be ``rnb''.
	
	\begin{figure}[!htb]
		\centering
		\includegraphics[width=\columnwidth]{plots/latin_rnb_similarity.pdf}
		\caption
		{Projection of feature 1 vs. feature 9 for 5 songs from the ``latin'' genre and 5 songs from the ``rnb'' genre. The distributions are very similar.}
		\label{fig:latin_rnb_similarity}
	\end{figure}
	
	Also, distributions with many outliers cannot easily be classified with a Gaussian. This can be seen in \autoref{fig:latin_extreme}, where two features were plotted against each other for ten songs from the ``latin'' genre.
	
	\begin{figure}[!htb]
		\centering
		\includegraphics[width=\columnwidth]{plots/latin_extreme.pdf}
		\caption
		{Projection of feature 1 vs. feature 2 for 10 songs from the ``latin'' genre. There are many extreme values for which kNN would have an easier time classifying than Gaussian.}
		\label{fig:latin_extreme}
	\end{figure}

	Interestingly, the Gaussian classifier does work relatively well for certain genres, such as ``classical''. This can be attributed to the fact that the ``classical'' data can be easily modeled with a Gaussian distribution. This can be seen for instance in \autoref{fig:classical_gaussian}, which shows a plot of two features for ten songs from the ``classical'' genre.
	
	% Examples of Gaussian working well: classical7
	
	\begin{figure}[!htb]
	\centering
	\includegraphics[width=\columnwidth]{plots/classical_gaussian.pdf}
	\caption
	{Projection of feature 0 vs. feature 7 for 10 songs from the ``classical'' genre. The distribution appears Gaussian.}
	\label{fig:classical_gaussian}
	\end{figure}

	\section*{Conclusion}
	
	The Gaussian and kNN classifiers were compared, showing the clear advantages of kNN for music genre classification. Possible further improvements to the work done in this assignment would be to implement a k-d tree from scratch or to refine the kNN algorithm, possibly using various kernel functions.
	
	% 
	
	%\renewcommand\refname{}
	%\bibliographystyle{unsrt}
	%\bibliography{readings}{}
	
\end{document}
